{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(points,m,c):\n",
    "    M = len(points)\n",
    "    total_cost = 0\n",
    "    for i in range(M):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        total_cost += (1/M) * ((y - m * x - c)**2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent\n",
    "def stochastic_step_gradient(points,learning_rate,m,c):\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        m = m - learning_rate * ((-2/M)*(y - m*x - c)*x)\n",
    "        c = c - learning_rate * ((-2/M)*(y - m*x - c))\n",
    "    return m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(points,learning_rate,m,c):\n",
    "    m_slope, c_slope = 0, 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        m_slope += (-2/M)*(y - m*x - c)*x\n",
    "        c_slope +=(-2/M)*(y - m*x - c)\n",
    "    new_m = m - learning_rate * m_slope\n",
    "    new_c = c - learning_rate * c_slope\n",
    "    return new_m,new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(points,learning_rate,num_iterations):\n",
    "    m, c,m1,c1 = 0,0,0,0\n",
    "    for i in range(num_iterations):\n",
    "        m, c = step_gradient(points,learning_rate,m, c)\n",
    "        m1,c1 = stochastic_step_gradient(points,learning_rate,m1,c1)\n",
    "        print(i, \"Cost of Batch Gradient Descent: \",cost(points,m,c))\n",
    "        print(i, \"Cost of Stochaistic Gradient Descent: \",cost(points,m1,c1))\n",
    "    return m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    points = np.loadtxt('data.csv',delimiter = \",\")\n",
    "    learning_rate = 0.0001\n",
    "    num_iterations = 10\n",
    "    m, c = gradient_descent(points,learning_rate,num_iterations)\n",
    "    print(\"Slope and Intercept are: \",m,c)\n",
    "    x = points[:,0]\n",
    "    y = points[:,1]\n",
    "    y_pred = m*x + c\n",
    "    u = ((y-y_pred)**2).sum()\n",
    "    v = ((y-y.mean())**2).sum()\n",
    "    score = 1 - (u/v)\n",
    "    print(\"Score with Gradient Descent is: \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost of Batch Gradient Descent:  1484.5865574086486\n",
      "0 Cost of Stochaistic Gradient Descent:  2106.5392947761416\n",
      "1 Cost of Batch Gradient Descent:  457.8542575737672\n",
      "1 Cost of Stochaistic Gradient Descent:  838.7592704985236\n",
      "2 Cost of Batch Gradient Descent:  199.5099857255389\n",
      "2 Cost of Stochaistic Gradient Descent:  375.24684837845876\n",
      "3 Cost of Batch Gradient Descent:  134.50591058200533\n",
      "3 Cost of Stochaistic Gradient Descent:  206.51781164815262\n",
      "4 Cost of Batch Gradient Descent:  118.1496934223995\n",
      "4 Cost of Stochaistic Gradient Descent:  145.54573787919594\n",
      "5 Cost of Batch Gradient Descent:  114.0341490603815\n",
      "5 Cost of Stochaistic Gradient Descent:  123.78825665772355\n",
      "6 Cost of Batch Gradient Descent:  112.99857731713657\n",
      "6 Cost of Stochaistic Gradient Descent:  116.19440981124372\n",
      "7 Cost of Batch Gradient Descent:  112.73798187568467\n",
      "7 Cost of Stochaistic Gradient Descent:  113.65044439538873\n",
      "8 Cost of Batch Gradient Descent:  112.6723843590911\n",
      "8 Cost of Stochaistic Gradient Descent:  112.8662968970805\n",
      "9 Cost of Batch Gradient Descent:  112.65585181499745\n",
      "9 Cost of Stochaistic Gradient Descent:  112.6699632042208\n",
      "Slope and Intercept are:  1.47741737554838 0.029639347874732384\n",
      "Score with Gradient Descent is:  0.5899252072394492\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.32243102] 7.991020982270399\n",
      "Score with inbuilt algorithm:  0.598655791538662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.loadtxt('data.csv',delimiter = \",\")\n",
    "alg1 = LinearRegression()\n",
    "alg1.fit(X[:,:-1].reshape(-1,1),X[:,-1])\n",
    "print(alg1.coef_,alg1.intercept_)\n",
    "# print(alg1.predict(X[:,:-1].reshape(-1,1)))\n",
    "print(\"Score with inbuilt algorithm: \",alg1.score(X[:,:-1].reshape(-1,1),X[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_cost(points,m,c):\n",
    "    M = len(points)\n",
    "    total_cost = 0\n",
    "    for i in range(M):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        total_cost += (1/M) * ((y - (m * x).sum() - c)**2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_step_gradient_descent(points,learning_rate,m,c):\n",
    "    N = points.shape[1]-1\n",
    "    m_slope, c_slope = np.zeros(N), 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, :-1]\n",
    "        y = points[i, -1]\n",
    "        for j in range(N):\n",
    "            m_slope[j] += (-2/M) * (y - (m*x).sum() - c)*x[j]\n",
    "        c_slope += (-2/M) * (y - (m*x).sum() - c)\n",
    "    new_m = m - learning_rate*m_slope\n",
    "    new_c = c - learning_rate*c_slope\n",
    "    return new_m,new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Gradient Descent\n",
    "def generic_gradient_descent(points,learning_rate,num_iterations):\n",
    "    N = points.shape[1]-1\n",
    "    m, c = np.zeros(N) , 0\n",
    "    for i in range(num_iterations):\n",
    "        m, c = generic_step_gradient_descent(points,learning_rate,m,c)\n",
    "        print(i,\"Cost: \", generic_cost(points,m,c))\n",
    "    return m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generic():\n",
    "    # getting the training data\n",
    "    dataset = np.genfromtxt(\"0000000000002329_training_diabetes_x_y_train.csv\",delimiter = \",\")\n",
    "    learning_rate = 0.0001\n",
    "    num_iterations = 10\n",
    "    m, c = generic_gradient_descent(dataset,learning_rate,num_iterations)\n",
    "    print(\"Coefficients are: \",m)\n",
    "    print(\"Intercept is: \",c)\n",
    "#     y_pred = m*x + c\n",
    "#     print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  0.0031732388825565054\n",
      "1 Cost:  0.005878385646927491\n",
      "2 Cost:  0.010375041718410248\n",
      "3 Cost:  0.016662132094447887\n",
      "4 Cost:  0.024738582274136858\n",
      "5 Cost:  0.034603318258012\n",
      "6 Cost:  0.0462552665478313\n",
      "7 Cost:  0.05969335414636167\n",
      "8 Cost:  0.07491650855716407\n",
      "9 Cost:  0.09192365778437837\n",
      "Coefficients are:  [ 0.00103733  0.00015081  0.00441378  0.00257135  0.00103555  0.00082749\n",
      " -0.00246147  0.00236916  0.00346693  0.00282222]\n",
      "Intercept is:  0.2991747662597829\n"
     ]
    }
   ],
   "source": [
    "run_generic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84852814 -0.34641016]\n",
      " [ 1.69705627  0.11547005]\n",
      " [-0.28284271  1.5011107 ]\n",
      " [-0.56568542 -1.27017059]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling using scale method\n",
    "from sklearn import preprocessing\n",
    "data = [[0,3], [9,4], [2,7], [1,1]]\n",
    "data_scaled1 = preprocessing.scale(data)\n",
    "print(data_scaled1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84852814 -0.34641016]\n",
      " [ 1.69705627  0.11547005]\n",
      " [-0.28284271  1.5011107 ]\n",
      " [-0.56568542 -1.27017059]]\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling using StandardScaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(data)\n",
    "data_scaled2 = scaler.transform(data)\n",
    "print(data_scaled2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
